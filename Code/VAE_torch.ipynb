{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76313c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try torch, the right py-kernel is python (myenv) and the env in conda is called ENVvae2, \n",
    "# more details can be found in the meeting notes with Lukas may 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the tutorial: https://www.kaggle.com/code/schmiddey/variational-autoencoder-with-pytorch-vs-pca\n",
    "# based on tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e230f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0399a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10241866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sampleID  year  wave  gender  age  annual_income  income_recode  \\\n",
      "0          1991_3  1991     0       1   21            NaN            2.0   \n",
      "1          1991_4  1991     0       2   32            NaN            8.0   \n",
      "2          1991_9  1991     0       2   48            NaN            8.0   \n",
      "3         1991_10  1991     0       1   35            NaN            8.0   \n",
      "4         1991_11  1991     0       1   48            NaN            6.0   \n",
      "...           ...   ...   ...     ...  ...            ...            ...   \n",
      "18820  1991_53593  1991     0       1   23            NaN            6.0   \n",
      "18821  1991_53594  1991     0       2   26            NaN            6.0   \n",
      "18822  1991_53595  1991     0       1   32            NaN            5.0   \n",
      "18823  1991_53616  1991     0       1   85            NaN            3.0   \n",
      "18824  1991_53618  1991     0       2   79            NaN            2.0   \n",
      "\n",
      "       education  edu  ethnicity  ...  status  ucod_leading  diabetes  \\\n",
      "0            3.0  2.0          3  ...     0.0           NaN       NaN   \n",
      "1            5.0  4.0          3  ...     0.0           NaN       NaN   \n",
      "2            5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "3            5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "4            1.0  1.0          3  ...     1.0           7.0       1.0   \n",
      "...          ...  ...        ...  ...     ...           ...       ...   \n",
      "18820        5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "18821        3.0  2.0          1  ...     0.0           NaN       NaN   \n",
      "18822        2.0  1.0          3  ...     0.0           NaN       NaN   \n",
      "18823        2.0  1.0          1  ...     0.0          10.0       0.0   \n",
      "18824        1.0  1.0          1  ...     1.0           2.0       0.0   \n",
      "\n",
      "       hyperten  permth_int   time       kdm0  kdm_advance0  phenoage0  \\\n",
      "0           NaN       293.0  240.0        NaN           NaN        NaN   \n",
      "1           NaN       317.0  240.0  31.125042     -0.874958        NaN   \n",
      "2           NaN       293.0  240.0  47.930040     -0.069960  41.526995   \n",
      "3           NaN       306.0  240.0  35.123927      0.123927  33.376651   \n",
      "4           0.0       206.0  206.0  53.160663      5.160663        NaN   \n",
      "...         ...         ...    ...        ...           ...        ...   \n",
      "18820       NaN       270.0  240.0        NaN           NaN        NaN   \n",
      "18821       NaN       261.0  240.0        NaN           NaN  24.815918   \n",
      "18822       NaN       273.0    NaN        NaN           NaN        NaN   \n",
      "18823       0.0       108.0  108.0        NaN           NaN        NaN   \n",
      "18824       0.0        43.0    NaN        NaN           NaN        NaN   \n",
      "\n",
      "       phenoage_advance0  \n",
      "0                    NaN  \n",
      "1                    NaN  \n",
      "2              -6.473005  \n",
      "3              -1.623349  \n",
      "4                    NaN  \n",
      "...                  ...  \n",
      "18820                NaN  \n",
      "18821          -1.184082  \n",
      "18822                NaN  \n",
      "18823                NaN  \n",
      "18824                NaN  \n",
      "\n",
      "[18825 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/Users/karolinagustavsson/Code/Python_VAE/NH3.csv\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af30ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e8a024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (9072, 10)\n",
      "Validation data shape: (3025, 10)\n",
      "Test data shape: (3025, 10)\n",
      "            albumin           alp         lymph           mcv       lncreat  \\\n",
      "count  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04   \n",
      "mean  -3.458269e-16  1.202876e-16 -3.007191e-17  8.570493e-16  5.415292e-16   \n",
      "std    1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00   \n",
      "min   -5.163708e+00 -2.539327e+00 -3.230490e+00 -5.518384e+00 -4.379853e+00   \n",
      "25%   -6.335773e-01 -6.675635e-01 -6.848479e-01 -5.169043e-01 -7.741436e-01   \n",
      "50%   -1.006212e-01 -1.537462e-01 -6.702905e-02  5.840629e-02 -2.969089e-01   \n",
      "75%    6.988140e-01  5.068761e-01  6.194364e-01  6.058776e-01  5.846864e-01   \n",
      "max    5.228944e+00  6.525879e+00  4.526568e+00  5.217632e+00  6.217355e+00   \n",
      "\n",
      "              lncrp         hba1c           wbc           rdw           age  \n",
      "count  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  \n",
      "mean  -2.556112e-16 -2.856831e-16  2.067443e-17 -7.621348e-16 -9.021572e-17  \n",
      "std    1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  \n",
      "min   -5.249485e-01 -2.887302e+00 -2.411500e+00 -2.245033e+00 -1.449208e+00  \n",
      "25%   -5.249485e-01 -5.436174e-01 -6.986163e-01 -6.564834e-01 -8.861164e-01  \n",
      "50%   -5.249485e-01 -1.174927e-01 -1.276548e-01 -2.097041e-01 -2.206439e-01  \n",
      "75%    1.772039e-01  2.021004e-01  5.622570e-01  4.356448e-01  8.543501e-01  \n",
      "max    5.867563e+00  6.167845e+00  5.439219e+00  6.343062e+00  2.134105e+00  \n",
      "        albumin       alp     lymph       mcv   lncreat     lncrp     hba1c  \\\n",
      "4493  -0.633577 -0.337252  0.928346 -0.405553 -1.279466  3.535414 -0.863211   \n",
      "7771   2.031205 -0.740966  0.945508  0.160476  1.757599 -0.524948 -0.650148   \n",
      "9345  -0.367099 -0.153746 -0.364497 -1.444823  0.155196  0.602316  0.521694   \n",
      "9139  -0.100621  0.690382  0.539349 -0.962304  0.993716 -0.524948  0.308632   \n",
      "11945 -1.166534  0.103162 -0.702009 -0.776720 -0.774144  0.341914  0.308632   \n",
      "\n",
      "            wbc       rdw       age  \n",
      "4493   0.538467 -1.053620 -0.834926  \n",
      "7771   0.776367 -0.308988 -0.886116  \n",
      "9345  -0.674826  0.435645  1.622203  \n",
      "9139  -0.532086 -0.755768  1.161491  \n",
      "11945  1.228378  0.137792  0.137687  \n"
     ]
    }
   ],
   "source": [
    "# apparently you have to restart and clear output \n",
    "# Define the columns of interest\n",
    "columns_of_interest = [\"sampleID\", \"status\", \"albumin\", \"alp\", \"lymph\", \"mcv\",\n",
    "                       \"lncreat\", \"lncrp\", \"hba1c\", \"wbc\", \"rdw\", \"age\"]\n",
    "\n",
    "# Select only the necessary columns\n",
    "df = df[columns_of_interest]\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep the ID and status columns separate if needed later\n",
    "ids = df['sampleID']\n",
    "status = df['status']\n",
    "\n",
    "# Remove these from the dataframe to be normalized\n",
    "df = df.drop(['sampleID', 'status'], axis=1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the data\n",
    "data_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Optionally convert back to DataFrame\n",
    "df_normalized = pd.DataFrame(data_normalized, columns=df.columns)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data\n",
    "x_train, x_test = train_test_split(df_normalized, test_size=0.2, random_state=123)\n",
    "x_train, x_val = train_test_split(x_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# Check the shapes of the resulting data splits\n",
    "print(\"Train data shape:\", x_train.shape)\n",
    "print(\"Validation data shape:\", x_val.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "\n",
    "print(df_normalized.describe())\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e055110",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wine notebook: https://www.kaggle.com/code/schmiddey/variational-autoencoder-with-pytorch-vs-pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e949e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab78ffc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445395f9",
   "metadata": {},
   "source": [
    "########################## Ã¤ndra\n",
    "def load_data(path):\n",
    "    # read in from csv\n",
    "    df = pd.read_csv(DATA_PATH, sep=',', header=None, names=['Wine', 'Alcohol','Malic.acid','Ash','Acl',\n",
    "                                                    'Mg', 'Phenols', 'Flavanoids','Nonflavanoid.phenols',\n",
    "                                                    'Proanth','Color.int','Hue', 'OD','Proline'])\n",
    "    # replace nan with -99\n",
    "    df = df.fillna(-99)\n",
    "    df_base = df.iloc[:, 1:]\n",
    "    # get wine Label\n",
    "    df_wine = df.iloc[:,0].values\n",
    "    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n",
    "    # stadardize values\n",
    "    standardizer = preprocessing.StandardScaler()\n",
    "    x = standardizer.fit_transform(x)    \n",
    "    return x, standardizer, df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25a51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data(path):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Define the columns of interest\n",
    "    columns_of_interest = [\"sampleID\", \"status\", \"albumin\", \"alp\", \"lymph\", \"mcv\",\n",
    "                           \"lncreat\", \"lncrp\", \"hba1c\", \"wbc\", \"rdw\", \"age\"]\n",
    "    \n",
    "    # Select only the necessary columns\n",
    "    df = df[columns_of_interest]\n",
    "    \n",
    "    # Remove rows with any missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Separate out IDs and status, and keep them for later use\n",
    "    ids = df['sampleID']\n",
    "    status = df['status']\n",
    "    \n",
    "    # Remove the ID and status columns from the dataframe to be normalized\n",
    "    df_normalized = df.drop(['sampleID', 'status'], axis=1)\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    x_normalized = scaler.fit_transform(df_normalized)\n",
    "    \n",
    "    # Convert normalized data back into a DataFrame with appropriate column names\n",
    "    df_normalized = pd.DataFrame(x_normalized, columns=df_normalized.columns)\n",
    "    \n",
    "    # Return the normalized data and scaler, along with IDs and status for reintegration later\n",
    "    return df_normalized, scaler, ids, status\n",
    "\n",
    "# Example usage\n",
    "DATA_PATH = '/Users/karolinagustavsson/Code/Python_VAE/NH3.csv'\n",
    "x_train, standardizer, train_ids, train_status = load_data(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63b1925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyToTensor(x):\n",
    "    x_train = torch.from_numpy(x).to(device)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc16c8",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.x, self.standardizer, self.wine = load_data(DATA_PATH)\n",
    "        self.x = numpyToTensor(self.x)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b4dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # Load the data and ignore IDs and status for training purposes\n",
    "        self.x, self.standardizer, self.ids, self.status = load_data(path)\n",
    "        # Convert the dataframe to a PyTorch tensor\n",
    "        self.x = torch.tensor(self.x.values, dtype=torch.float32)\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return the tensor at the specified index\n",
    "        return self.x[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccbba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=DataBuilder(DATA_PATH)\n",
    "trainloader=DataLoader(dataset=data_set,batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b076118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainloader.dataset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ebb2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4982,  0.2133,  0.3334,  ..., -0.2466, -0.8054, -1.3980],\n",
       "        [-0.1006,  0.1032, -0.4331,  ...,  2.5606, -0.0608, -0.8349],\n",
       "        [-0.3671, -0.9979, -0.0613,  ..., -1.0555, -1.1033, -0.0159],\n",
       "        ...,\n",
       "        [ 1.7647,  0.9106, -1.2798,  ...,  0.3006,  0.1378, -1.2956],\n",
       "        [-0.3671, -0.5942,  1.5805,  ..., -0.9603,  0.8328, -1.1421],\n",
       "        [ 0.4323,  1.9749, -0.1586,  ..., -0.2942,  0.5349,  1.8782]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "702b302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build model and train it\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "#         # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "#         # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "#         # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # when the forward method is called, the decode method's output is the reconstructed batch (recon_batch), and \n",
    "        #the outputs mu and logvar are the parameters of the latent distribution used for sampling\n",
    "        return self.decode(z), mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a812f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5fdd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa2c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = data_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "model = Autoencoder(D_in, H, H2).to(device)\n",
    "model.apply(weights_init_uniform_rule)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f843923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = customLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e64783c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model\n",
    "epochs = 10\n",
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6374d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "#        if batch_idx % log_interval == 0:\n",
    "#            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "#                       100. * batch_idx / len(trainloader),\n",
    "#                       loss.item() / len(data)))\n",
    "    if epoch % 200 == 0:        \n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14209bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85840054",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate\n",
    "standardizer = trainloader.dataset.standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b9264c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "# no_grad() bedeutet wir nehmen die vorher berechneten Gewichte und erneuern sie nicht\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12066b68",
   "metadata": {},
   "source": [
    "standardizer.inverse_transform(recon_batch[65].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "160a2a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.3136525 , 76.45169   , 31.511707  , 92.713585  ,  0.68965113,\n",
       "         0.21598017,  5.0457606 ,  6.3450484 , 12.67099   , 58.881157  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizer.inverse_transform(recon_batch[65].cpu().numpy().reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dd981",
   "metadata": {},
   "source": [
    "standardizer.inverse_transform(data[5].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6eee7c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.1       , 55.        , 48.9       , 94.6       ,  0.51879376,\n",
       "         0.19062035,  5.        ,  4.55      , 12.7       , 26.999998  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizer.inverse_transform(data[65].cpu().numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee34585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get embeddings / latent variables\n",
    "mu_output = []\n",
    "logvar_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data) in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "\n",
    "            \n",
    "            mu_tensor = mu   \n",
    "            mu_output.append(mu_tensor)\n",
    "            mu_result = torch.cat(mu_output, dim=0)\n",
    "\n",
    "            logvar_tensor = logvar   \n",
    "            logvar_output.append(logvar_tensor)\n",
    "            logvar_result = torch.cat(logvar_output, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9fb991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15122, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba21062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0218,  0.7471, -1.1497],\n",
       "        [-0.1517, -0.1463,  0.1582],\n",
       "        [-0.1189, -0.1455,  0.1300],\n",
       "        [ 1.3931,  1.0089, -0.7016]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_result[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b64ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot embeddings of VAE\n",
    "#ax = plt.axes(projection='3d')\n",
    "\n",
    "\n",
    "# Data for three-dimensional scattered points\n",
    "#winetype = data_set.wine\n",
    "#zdata = mu_result[:,0].cpu().numpy()\n",
    "#xdata = mu_result[:,1].cpu().numpy()\n",
    "#ydata = mu_result[:,2].cpu().numpy()\n",
    "#ax.scatter3D(xdata, ydata, zdata, c=winetype);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afb16a",
   "metadata": {},
   "source": [
    "So now it works but it is time to make a 2.0 version "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "envvae2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
