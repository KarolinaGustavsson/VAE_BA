{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a70625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a40a0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# måste vara python 3 iom att tf är kompatibel med 3.9 och inte 3.12 råkade installera \n",
    "# miljön ENVvae2 s kernel med namet Python (myenv) oh well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2c21cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sampleID  year  wave  gender  age  annual_income  income_recode  \\\n",
      "0          1991_3  1991     0       1   21            NaN            2.0   \n",
      "1          1991_4  1991     0       2   32            NaN            8.0   \n",
      "2          1991_9  1991     0       2   48            NaN            8.0   \n",
      "3         1991_10  1991     0       1   35            NaN            8.0   \n",
      "4         1991_11  1991     0       1   48            NaN            6.0   \n",
      "...           ...   ...   ...     ...  ...            ...            ...   \n",
      "18820  1991_53593  1991     0       1   23            NaN            6.0   \n",
      "18821  1991_53594  1991     0       2   26            NaN            6.0   \n",
      "18822  1991_53595  1991     0       1   32            NaN            5.0   \n",
      "18823  1991_53616  1991     0       1   85            NaN            3.0   \n",
      "18824  1991_53618  1991     0       2   79            NaN            2.0   \n",
      "\n",
      "       education  edu  ethnicity  ...  status  ucod_leading  diabetes  \\\n",
      "0            3.0  2.0          3  ...     0.0           NaN       NaN   \n",
      "1            5.0  4.0          3  ...     0.0           NaN       NaN   \n",
      "2            5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "3            5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "4            1.0  1.0          3  ...     1.0           7.0       1.0   \n",
      "...          ...  ...        ...  ...     ...           ...       ...   \n",
      "18820        5.0  4.0          1  ...     0.0           NaN       NaN   \n",
      "18821        3.0  2.0          1  ...     0.0           NaN       NaN   \n",
      "18822        2.0  1.0          3  ...     0.0           NaN       NaN   \n",
      "18823        2.0  1.0          1  ...     0.0          10.0       0.0   \n",
      "18824        1.0  1.0          1  ...     1.0           2.0       0.0   \n",
      "\n",
      "       hyperten  permth_int   time       kdm0  kdm_advance0  phenoage0  \\\n",
      "0           NaN       293.0  240.0        NaN           NaN        NaN   \n",
      "1           NaN       317.0  240.0  31.125042     -0.874958        NaN   \n",
      "2           NaN       293.0  240.0  47.930040     -0.069960  41.526995   \n",
      "3           NaN       306.0  240.0  35.123927      0.123927  33.376651   \n",
      "4           0.0       206.0  206.0  53.160663      5.160663        NaN   \n",
      "...         ...         ...    ...        ...           ...        ...   \n",
      "18820       NaN       270.0  240.0        NaN           NaN        NaN   \n",
      "18821       NaN       261.0  240.0        NaN           NaN  24.815918   \n",
      "18822       NaN       273.0    NaN        NaN           NaN        NaN   \n",
      "18823       0.0       108.0  108.0        NaN           NaN        NaN   \n",
      "18824       0.0        43.0    NaN        NaN           NaN        NaN   \n",
      "\n",
      "       phenoage_advance0  \n",
      "0                    NaN  \n",
      "1                    NaN  \n",
      "2              -6.473005  \n",
      "3              -1.623349  \n",
      "4                    NaN  \n",
      "...                  ...  \n",
      "18820                NaN  \n",
      "18821          -1.184082  \n",
      "18822                NaN  \n",
      "18823                NaN  \n",
      "18824                NaN  \n",
      "\n",
      "[18825 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"/Users/karolinagustavsson/Code/Python_VAE/NH3.csv\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e09022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3170de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns of interest\n",
    "columns_of_interest = [\"sampleID\", \"status\", \"albumin\", \"alp\", \"lymph\", \"mcv\",\n",
    "                       \"lncreat\", \"lncrp\", \"hba1c\", \"wbc\", \"rdw\", \"age\"]\n",
    "\n",
    "# Select only the necessary columns\n",
    "df = df[columns_of_interest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca1f1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2025c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the ID and status columns separate if needed later\n",
    "ids = df['sampleID']\n",
    "status = df['status']\n",
    "\n",
    "# Remove these from the dataframe to be normalized\n",
    "df = df.drop(['sampleID', 'status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18633f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize the data\n",
    "data_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Optionally convert back to DataFrame\n",
    "df_normalized = pd.DataFrame(data_normalized, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8d44407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (9072, 10)\n",
      "Validation data shape: (3025, 10)\n",
      "Test data shape: (3025, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data\n",
    "x_train, x_test = train_test_split(df_normalized, test_size=0.2, random_state=123)\n",
    "x_train, x_val = train_test_split(x_train, test_size=0.25, random_state=123)\n",
    "\n",
    "\n",
    "# Check the shapes of the resulting data splits\n",
    "print(\"Train data shape:\", x_train.shape)\n",
    "print(\"Validation data shape:\", x_val.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ca11f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            albumin           alp         lymph           mcv       lncreat  \\\n",
      "count  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04   \n",
      "mean  -3.458269e-16  1.202876e-16 -3.007191e-17  8.570493e-16  5.415292e-16   \n",
      "std    1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00   \n",
      "min   -5.163708e+00 -2.539327e+00 -3.230490e+00 -5.518384e+00 -4.379853e+00   \n",
      "25%   -6.335773e-01 -6.675635e-01 -6.848479e-01 -5.169043e-01 -7.741436e-01   \n",
      "50%   -1.006212e-01 -1.537462e-01 -6.702905e-02  5.840629e-02 -2.969089e-01   \n",
      "75%    6.988140e-01  5.068761e-01  6.194364e-01  6.058776e-01  5.846864e-01   \n",
      "max    5.228944e+00  6.525879e+00  4.526568e+00  5.217632e+00  6.217355e+00   \n",
      "\n",
      "              lncrp         hba1c           wbc           rdw           age  \n",
      "count  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  1.512200e+04  \n",
      "mean  -2.556112e-16 -2.856831e-16  2.067443e-17 -7.621348e-16 -9.021572e-17  \n",
      "std    1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  1.000033e+00  \n",
      "min   -5.249485e-01 -2.887302e+00 -2.411500e+00 -2.245033e+00 -1.449208e+00  \n",
      "25%   -5.249485e-01 -5.436174e-01 -6.986163e-01 -6.564834e-01 -8.861164e-01  \n",
      "50%   -5.249485e-01 -1.174927e-01 -1.276548e-01 -2.097041e-01 -2.206439e-01  \n",
      "75%    1.772039e-01  2.021004e-01  5.622570e-01  4.356448e-01  8.543501e-01  \n",
      "max    5.867563e+00  6.167845e+00  5.439219e+00  6.343062e+00  2.134105e+00  \n",
      "        albumin       alp     lymph       mcv   lncreat     lncrp     hba1c  \\\n",
      "4493  -0.633577 -0.337252  0.928346 -0.405553 -1.279466  3.535414 -0.863211   \n",
      "7771   2.031205 -0.740966  0.945508  0.160476  1.757599 -0.524948 -0.650148   \n",
      "9345  -0.367099 -0.153746 -0.364497 -1.444823  0.155196  0.602316  0.521694   \n",
      "9139  -0.100621  0.690382  0.539349 -0.962304  0.993716 -0.524948  0.308632   \n",
      "11945 -1.166534  0.103162 -0.702009 -0.776720 -0.774144  0.341914  0.308632   \n",
      "\n",
      "            wbc       rdw       age  \n",
      "4493   0.538467 -1.053620 -0.834926  \n",
      "7771   0.776367 -0.308988 -0.886116  \n",
      "9345  -0.674826  0.435645  1.622203  \n",
      "9139  -0.532086 -0.755768  1.161491  \n",
      "11945  1.228378  0.137792  0.137687  \n"
     ]
    }
   ],
   "source": [
    "print(df_normalized.describe())\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df8ede85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2f91ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "def build_encoder():\n",
    "    inputs = keras.Input(shape=(original_dim,))\n",
    "    x = layers.Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "    x = layers.Dense(intermediate_dim2, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    return Model(inputs, [z_mean, z_log_var], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "846dc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "def build_decoder():\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(intermediate_dim2, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(intermediate_dim1, activation='relu')(x)\n",
    "    outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "    return Model(latent_inputs, outputs, name='decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee5c8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = z_mean.shape[1]  # Assure this aligns with latent_dim\n",
    "    epsilon = tf.random.normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d554cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def compute_loss(self, x, reconstructed_x, z_log_var, z_mean):\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(binary_crossentropy(x, reconstructed_x), axis=(1, 2))\n",
    "        )\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        return reconstruction_loss + kl_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = sampling([z_mean, z_log_var])\n",
    "            reconstruction = self.decoder(z)\n",
    "            loss = self.compute_loss(data, reconstruction, z_log_var, z_mean)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {'loss': loss}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3921c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 17:28:00.299665: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (1 for input with 1 dimension(s) [Op:Sum]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch \u001b[38;5;129;01min\u001b[39;00m batch_data(x_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m         loss_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_metrics)\n",
      "Cell \u001b[0;32mIn[58], line 19\u001b[0m, in \u001b[0;36mVAE.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     16\u001b[0m z \u001b[38;5;241m=\u001b[39m sampling([z_mean, z_log_var])\n\u001b[1;32m     17\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m     18\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Sum_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid reduction dimension (1 for input with 1 dimension(s) [Op:Sum]"
     ]
    }
   ],
   "source": [
    "# Model dimensions\n",
    "original_dim = df_normalized.shape[1]\n",
    "latent_dim = 9\n",
    "intermediate_dim1 = 128\n",
    "intermediate_dim2 = 64\n",
    "\n",
    "# Custom loss function that integrates with Keras\n",
    "def vae_loss(x, reconstructed_x):\n",
    "    z_mean, z_log_var = vae.encoder(x)\n",
    "    return vae.compute_loss(x, reconstructed_x, z_log_var, z_mean)\n",
    "\n",
    "# Compile the VAE with the custom loss\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "def batch_data(dataset, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(buffer_size=1024).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    for x_batch in batch_data(x_train, batch_size=32):\n",
    "        loss_metrics = vae.train_step(x_batch)\n",
    "    print(f'Epoch {epoch}:', loss_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb9f595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 319us/step - loss: 0.0000e+00\n",
      "Test Loss: 0.0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "New data samples: [[0.55860984 0.53282857 0.37915173 0.4561452  0.5531349  0.5954364\n",
      "  0.51232916 0.5835415  0.5637799  0.5047507 ]\n",
      " [0.48879308 0.5051854  0.4478212  0.47192594 0.5647013  0.5280689\n",
      "  0.5143723  0.6401957  0.54843843 0.5094463 ]\n",
      " [0.5545277  0.5165425  0.43397313 0.46396014 0.60363746 0.5794969\n",
      "  0.50005966 0.53651506 0.530094   0.5965761 ]\n",
      " [0.5676327  0.45990363 0.23049577 0.4609862  0.62444985 0.56325454\n",
      "  0.6418303  0.7071232  0.61610514 0.6097236 ]\n",
      " [0.4694941  0.495997   0.44652924 0.42531186 0.5350577  0.55273247\n",
      "  0.47954285 0.6325082  0.46073848 0.47887534]\n",
      " [0.5531708  0.47942814 0.40822694 0.37030256 0.53060174 0.5814584\n",
      "  0.4612896  0.5897552  0.50618374 0.5618958 ]\n",
      " [0.5501539  0.65529203 0.37962127 0.4019643  0.6124212  0.70264435\n",
      "  0.4226347  0.6684646  0.44491452 0.5558268 ]\n",
      " [0.50199693 0.5208499  0.38880768 0.39453387 0.52031046 0.55123013\n",
      "  0.49896842 0.5914719  0.47675714 0.58419776]\n",
      " [0.5335791  0.49400687 0.4245648  0.43152037 0.58607846 0.55852354\n",
      "  0.42961532 0.5330922  0.5172615  0.6316123 ]\n",
      " [0.5949384  0.4666724  0.404237   0.39378175 0.53189224 0.59337837\n",
      "  0.47332555 0.55470574 0.5518963  0.6156449 ]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = vae.evaluate(batch_data(x_test, batch_size=32), return_dict=True)\n",
    "print('Test Loss:', test_loss['loss'])\n",
    "\n",
    "# Generate new data\n",
    "z_sample = np.random.normal(size=(10, latent_dim))\n",
    "new_data = vae.decoder.predict(z_sample)\n",
    "print(\"New data samples:\", new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3171a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5681bed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 87)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampleID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 14\u001b[0m data_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m x_train, x_test \u001b[38;5;241m=\u001b[39m train_test_split(data_normalized, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m     16\u001b[0m x_train, x_val \u001b[38;5;241m=\u001b[39m train_test_split(x_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/sklearn/utils/validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    938\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 87)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"/Users/karolinagustavsson/Code/Python_VAE/NH3.csv\")\n",
    "df = df.dropna().drop(columns=['sampleID', 'status'])\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(df)\n",
    "x_train, x_test = train_test_split(data_normalized, test_size=0.2, random_state=123)\n",
    "x_train, x_val = train_test_split(x_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# Model dimensions\n",
    "original_dim = df.shape[1]\n",
    "latent_dim = 9\n",
    "intermediate_dim1 = 128\n",
    "intermediate_dim2 = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Build the encoder and decoder\n",
    "encoder_inputs = keras.Input(shape=(original_dim,))\n",
    "x = layers.Dense(intermediate_dim1, activation='relu')(encoder_inputs)\n",
    "x = layers.Dense(intermediate_dim2, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "encoder = Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(intermediate_dim2, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(intermediate_dim1, activation='relu')(x)\n",
    "decoder_outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "# VAE class with custom methods\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def vae_loss(self, x, reconstructed_x):\n",
    "        reconstruction_loss = binary_crossentropy(x, reconstructed_x) * original_dim\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer='adam', loss=vae.vae_loss)\n",
    "\n",
    "# Train the VAE\n",
    "history = vae.fit(x_train, x_train, epochs=50, batch_size=batch_size, validation_data=(x_val, x_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fa4ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/var/folders/3w/hr_sr9_55818skvwsbdfyhzr0000gn/T/ipykernel_74555/2274926753.py\", line 89, in train_step\n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mean_squared_error(data, reconstruction), axis=1))\n\n    ValueError: Invalid reduction dimension 1 for input with 1 dimensions. for '{{node Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](Mean, Sum/reduction_indices)' with input shapes: [?], [] and with computed input tensors: input[1] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mvae\u001b[38;5;241m.\u001b[39mtrain_step)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Train the VAE\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vae\u001b[38;5;241m.\u001b[39mevaluate(x_test))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/3w/hr_sr9_55818skvwsbdfyhzr0000gn/T/__autograph_generated_filelhy52gyz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[80], line 89\u001b[0m, in \u001b[0;36mVAE.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m z \u001b[38;5;241m=\u001b[39m sampling([z_mean, z_log_var])\n\u001b[1;32m     88\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m---> 89\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mreduce_sum(keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mmean_squared_error(data, reconstruction), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     90\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/Caskroom/miniforge/base/envs/ENVvae2/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/var/folders/3w/hr_sr9_55818skvwsbdfyhzr0000gn/T/ipykernel_74555/2274926753.py\", line 89, in train_step\n        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mean_squared_error(data, reconstruction), axis=1))\n\n    ValueError: Invalid reduction dimension 1 for input with 1 dimensions. for '{{node Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](Mean, Sum/reduction_indices)' with input shapes: [?], [] and with computed input tensors: input[1] = <1>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/Users/karolinagustavsson/Code/Python_VAE/NH3.csv\")\n",
    "\n",
    "# Define the columns of interest\n",
    "columns_of_interest = [\"sampleID\", \"status\", \"albumin\", \"alp\", \"lymph\", \"mcv\", \"lncreat\", \"lncrp\", \"hba1c\", \"wbc\", \"rdw\", \"age\"]\n",
    "\n",
    "# Select only the necessary columns and drop rows with missing values\n",
    "df = df[columns_of_interest].dropna()\n",
    "\n",
    "# Extract sampleID and status for later use and remove them from the dataframe\n",
    "ids = df.pop('sampleID')\n",
    "status = df.pop('status')\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "df_normalized = scaler.fit_transform(df)\n",
    "df_normalized = pd.DataFrame(df_normalized, columns=df.columns)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test = train_test_split(df_normalized, test_size=0.2, random_state=123)\n",
    "x_train, x_val = train_test_split(x_train, test_size=0.25, random_state=123)\n",
    "\n",
    "# Before you train, ensure the data type is correct\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Define the model dimensions\n",
    "original_dim = df_normalized.shape[1]\n",
    "latent_dim = 9  # Can vary this to find the optimal number of latent dimensions\n",
    "intermediate_dim1 = 128\n",
    "intermediate_dim2 = 64\n",
    "\n",
    "# Encoder network\n",
    "def build_encoder():\n",
    "    inputs = keras.Input(shape=(original_dim,))\n",
    "    x = layers.Dense(intermediate_dim1, activation='relu')(inputs)\n",
    "    x = layers.Dense(intermediate_dim2, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    model = Model(inputs, [z_mean, z_log_var], name='encoder')\n",
    "    return model\n",
    "\n",
    "# Decoder network\n",
    "def build_decoder():\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(intermediate_dim2, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(intermediate_dim1, activation='relu')(x)\n",
    "    outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "    model = Model(latent_inputs, outputs, name='decoder')\n",
    "    return model\n",
    "\n",
    "# Sampling function\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# VAE model class with correct loss function setup\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = sampling([z_mean, z_log_var])\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]  # Ensure data is unpacked correctly if it comes in a tuple form\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = sampling([z_mean, z_log_var])\n",
    "            reconstruction = self.decoder(z)\n",
    "            # Using Mean Squared Error for reconstruction loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.mean_squared_error(data, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {'loss': total_loss, 'reconstruction_loss': reconstruction_loss, 'kl_loss': kl_loss}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the encoder and decoder\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "\n",
    "# Initialize VAE\n",
    "vae.compile(optimizer='adam', loss=vae.train_step)\n",
    "\n",
    "# Train the VAE\n",
    "history = vae.fit(x_train, epochs=50, batch_size=32, validation_data=(x_val, x_val))\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Test Loss:\", vae.evaluate(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852f010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "envvae2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
